{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trainer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onry4ml0zJK7",
        "outputId": "79bff191-98c0-4017-83c0-6662b5528ab2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp24hTdwzKo3",
        "outputId": "76ab7aca-1b9d-4832-b956-aaa773d64061"
      },
      "source": [
        "%cd /content/drive/MyDrive/NowFoodFeedback/main"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NowFoodFeedback/main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY4eu8Af0JkI",
        "outputId": "fa484266-0f2c-4411-9bc1-a3db0cb4e8bc"
      },
      "source": [
        "#!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install ."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NowFoodFeedback/main/transformers\n",
            "Processing /content/drive/My Drive/NowFoodFeedback/main/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2352157 sha256=99ddbea4d9ee212a87306a91b841779cc0c912af9aaad6d27086dde823c761eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cpk9cz80/wheels/04/cd/b3/5a0b0a8d9cb0f77a670fa18b6bb0b93659d6f3699bcb2d4f7c\n",
            "Successfully built transformers\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBiibEUzq3V"
      },
      "source": [
        "!pip install fastBPE\n",
        "!pip install fairseq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvdCCzmP0Vfa"
      },
      "source": [
        "!pip install vncorenlp\n",
        "# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dSM72_L0paR"
      },
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "!tar -xzvf PhoBERT_base_transformers.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTOm2VX31ALt",
        "outputId": "dd09b381-ca8d-43b8-f5c3-ea70e82fd690"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NowFoodFeedback/main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t74J0mhszZNO",
        "outputId": "b1dfb11d-9131-4178-81d1-92434435b07b"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-05 10:20:56.674363: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Loading codes from transformers/PhoBERT_base_transformers/bpe.codes ...\n",
            "Read 64000 codes from the codes file.\n",
            "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "1680it [07:30,  3.73it/s]\n",
            " Accuracy: 0.4537\n",
            " F1 score: 0.3276\n",
            " Average training loss: 1.2665\n",
            "Running Validation...\n",
            "100% 560/560 [00:37<00:00, 15.13it/s]\n",
            " Accuracy: 0.5220\n",
            " F1 score: 0.4184\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "1680it [07:32,  3.71it/s]\n",
            " Accuracy: 0.5748\n",
            " F1 score: 0.4715\n",
            " Average training loss: 1.0306\n",
            "Running Validation...\n",
            "100% 560/560 [00:36<00:00, 15.19it/s]\n",
            " Accuracy: 0.5577\n",
            " F1 score: 0.4706\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "1680it [07:34,  3.70it/s]\n",
            " Accuracy: 0.6760\n",
            " F1 score: 0.5921\n",
            " Average training loss: 0.8325\n",
            "Running Validation...\n",
            "100% 560/560 [00:36<00:00, 15.17it/s]\n",
            " Accuracy: 0.6292\n",
            " F1 score: 0.5503\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "1371it [06:10,  3.71it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaxZpqrJzilN",
        "outputId": "c33765ed-d5ba-4719-ba77-78107a068e2f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: LS: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIVHjU_rePfH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}